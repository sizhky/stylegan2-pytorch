{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stylegan-2-style-transfer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOWBQODTdTGI4GYpfVlUytt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sizhky/stylegan2-pytorch/blob/master/stylegan_2_style_transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXaRArz5R_Zh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "outputId": "1e381487-c934-4de5-9519-8354c917d1f7"
      },
      "source": [
        "%%time\n",
        "import os\n",
        "\n",
        "if not os.path.exists('stylegan2-pytorch'):\n",
        "    !git clone https://github.com/sizhky/stylegan2-pytorch\n",
        "    !wget --quiet https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "    !sudo unzip -q ninja-linux.zip -d /usr/local/bin/\n",
        "    !sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
        "    !rm ninja-linux.zip\n",
        "    !pip install -U -q PyDrive torch_snippets\n",
        "    from pydrive.auth import GoogleAuth\n",
        "    from pydrive.drive import GoogleDrive\n",
        "    from google.colab import auth\n",
        "    from oauth2client.client import GoogleCredentials\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth) \n",
        "    downloaded = drive.CreateFile({'id':\"1PQutd-JboOCOZqmd95XWxWrO8gGEvRcO\"})   # replace the id with id of file you want to access\n",
        "    downloaded.GetContentFile('5500000.pth')        # replace the file name with your file\n",
        "%cd stylegan2-pytorch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'stylegan2-pytorch'...\n",
            "remote: Enumerating objects: 40, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 348 (delta 14), reused 0 (delta 0), pack-reused 308\u001b[K\n",
            "Receiving objects: 100% (348/348), 152.37 MiB | 34.68 MiB/s, done.\n",
            "Resolving deltas: 100% (163/163), done.\n",
            "update-alternatives: using /usr/local/bin/ninja to provide /usr/bin/ninja (ninja) in auto mode\n",
            "\u001b[K     |████████████████████████████████| 36.4MB 82kB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 6.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 10.4MB/s \n",
            "\u001b[?25h  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "/content/stylegan2-pytorch\n",
            "CPU times: user 13.9 s, sys: 3.14 s, total: 17.1 s\n",
            "Wall time: 1min 50s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1koJbacORiV",
        "colab_type": "text"
      },
      "source": [
        "### Playing with Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GY_jrNqLAU53",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "dfa0966e-4cf6-4ff3-8c69-f3328512e473"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from torch_snippets import *\n",
        "from generate import Generator\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "device = 'cuda'\n",
        "generator = Generator(size=256, style_dim=512, \n",
        "                      n_mlp=8, channel_multiplier=2)\n",
        "generator.load_state_dict(torch.load('../5500000.pth')['g_ema'], strict=False)\n",
        "generator.eval().to(device);"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/setuptools/distutils_patch.py:26: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
            "  \"Distutils was imported before Setuptools. This usage is discouraged \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2gc2gelu4Hc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def interpolate_two_points(p1, p2, n_steps=8):\n",
        "    ratios = torch.linspace(0, 1, steps=n_steps)\n",
        "    vectors = []\n",
        "    for ratio in ratios:\n",
        "        v = (1.0 - ratio) * p1 + ratio * p2\n",
        "        vectors.append(v)\n",
        "    return torch.stack(vectors)\n",
        "def interpolate_four_points(p1,p2,p3,p4,n_steps=8):\n",
        "    z1 = interpolate_two_points(p1, p2, n_steps)\n",
        "    z2 = interpolate_two_points(p3, p4, n_steps)\n",
        "    zs = []\n",
        "    for _z1,_z2 in zip(z1, z2):\n",
        "        zs.append(interpolate_two_points(_z1,_z2, n_steps))\n",
        "    zs = torch.cat(zs)\n",
        "    return zs\n",
        "device = 'cuda'\n",
        "with torch.no_grad():\n",
        "    steps = 8\n",
        "    # corner_zs = torch.randn(4, 14, 512, device=device)\n",
        "    # zs = interpolate_four_points(*corner_zs, steps)\n",
        "    # dumpdill(zs, 'noise.vectors')\n",
        "    '''Above three lines create the vectors in `noise.vectors` file\n",
        "    You can uncomment and create your own random ones\n",
        "    '''\n",
        "    zs = loaddill('noise.vectors')\n",
        "    zs = generator.get_latent(zs)\n",
        "    assert zs.shape == (64,14,512)\n",
        "    sample, _ = generator([zs])\n",
        "    save_image(sample, 'sample/interpolations.png', nrow=steps, normalize=True, range=(-1,1)) "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoK-0LfD2q4B",
        "colab_type": "text"
      },
      "source": [
        "In above snippet, notice `assert zs.shape == 64,14,512`. There are 64 styles for 64 images. Each style consists of **14** inputs that are fed at different stages in the network (some are near input layer, some in the middle and some are near output layer). Constraining exclusively those vectors which are near input layer cause a coarse transfer while constraining those near output layer cause a fine transfer of features... \n",
        "\n",
        "The only difference in each of the functions below is that we are sending peices of `target_style` vectors in each function (for coarse, it is `target_style[:,:4]` etc..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYqBg14QeOmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transfer_coarse_latent(source_styles, target_style):\n",
        "    originals, _ = generator(source_styles)\n",
        "    style_transferred, _ = generator(source_styles, \n",
        "                                  coarse_latents=target_style[:,:4].repeat(len(source_styles),1,1))\n",
        "    samples = torch.cat([originals, style_transferred], 0)\n",
        "    save_image(samples, f'sample/coarse_transfer_{name}.png', nrow=len(samples)//2, normalize=True, range=(-1,1))\n",
        "\n",
        "def transfer_middle_latent(source_styles, target_style):\n",
        "    originals, _ = generator(source_styles)\n",
        "    style_transferred, _ = generator(source_styles, \n",
        "                                  middle_latents=target_style[:,4:10].repeat(len(source_styles),1,1))\n",
        "    samples = torch.cat([originals, style_transferred], 0)\n",
        "    save_image(samples, f'sample/middle_transfer_{name}.png', nrow=len(samples)//2, normalize=True, range=(-1,1))\n",
        "\n",
        "\n",
        "def transfer_fine_latent(source_styles, target_style):\n",
        "    originals, _ = generator(source_styles)\n",
        "    style_transferred, _ = generator(source_styles, \n",
        "                                  fine_latents=target_style[:,10:].repeat(len(source_styles),1,1))\n",
        "    samples = torch.cat([originals, style_transferred], 0)\n",
        "    save_image(samples, f'sample/fine_transfer_{name}.png', nrow=len(samples)//2, normalize=True, range=(-1,1))\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tgy8t7HB4O7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we'll use these styles\n",
        "styles = {\n",
        "    'frontal-black-hair-female': zs[56],\n",
        "    'kid-worried': zs[39],\n",
        "    'bearded-man': zs[63]\n",
        "}\n",
        "\n",
        "z = torch.randn(10, 14, 512, device=device)\n",
        "z = generator.get_latent(z)\n",
        "for name, latent in styles.items():\n",
        "    z_ = torch.cat([latent[None], z])\n",
        "    transfer_coarse_latent([z_.clone()], latent[None].clone())\n",
        "    transfer_middle_latent([z_.clone()], latent[None].clone())\n",
        "    transfer_fine_latent(   [z_.clone()], latent[None].clone())\n",
        "# Visit /content/stylegan2-pytorch/sample/ folder to see the images"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcgUIkuPwwek",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9db509b2-4b26-4035-905f-b9a8388b7623"
      },
      "source": [
        "# or zip them and download \n",
        "import zipfile\n",
        "lista_files = Glob('sample')\n",
        "with zipfile.ZipFile('sample.zip', 'w') as zipMe:\n",
        "    for file in lista_files:\n",
        "        zipMe.write(file, compress_type=zipfile.ZIP_DEFLATED)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-01 21:19:15.609 | INFO     | torch_snippets.loader:Glob:150 - 10 files found at sample\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2CIsWK15I2A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}